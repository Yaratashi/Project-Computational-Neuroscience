{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Parameters \n",
    "num_input_neurons = 1000\n",
    "num_network_neurons = 250\n",
    "g_max = 0.02  \n",
    "A_plus = 0.001  \n",
    "A_minus = A_plus * 1.05  \n",
    "tau_plus = 20  \n",
    "tau_minus = 20  \n",
    "simulation_time = 1000  # Reduce for testing\n",
    "dt = 1  \n",
    "stimulus_start, stimulus_end = 601, 800  \n",
    "time_window = 100  \n",
    "\n",
    "# LIF Parameters\n",
    "C_m = 20  # Membrane capacitance (ms)\n",
    "V_rest = -74  # Resting potential (mV)\n",
    "V_thresh = -54  # Spiking threshold (mV)\n",
    "V_reset = -60  # Reset potential (mV)\n",
    "E_ex = 0  # Excitatory reversal potential (mV)\n",
    "tau_ex = 5  # Synaptic decay constant (ms)\n",
    "\n",
    "# Rate function parameters for input neurons\n",
    "R0 = 10  \n",
    "R1 = 80  \n",
    "sigma = 100  \n",
    "\n",
    "# Generate Corrected Spike Trains for Input Neurons (601-800)\n",
    "def generate_corrected_spike_train(neuron_id, duration):\n",
    "    spikes = []\n",
    "    for t in range(duration):\n",
    "        s = np.random.randint(stimulus_start, stimulus_end)  \n",
    "        rate = R0 + R1 * (np.exp(-((s - neuron_id)**2) / (2 * sigma**2)) +\n",
    "                          np.exp(-((s + 1000 - neuron_id)**2) / (2 * sigma**2)) +\n",
    "                          np.exp(-((s - 1000 - neuron_id)**2) / (2 * sigma**2)))\n",
    "        if np.random.rand() < (rate * dt / 1000):  \n",
    "            spikes.append(t)\n",
    "    return np.array(spikes)\n",
    "\n",
    "input_spikes = {i: generate_corrected_spike_train(i, simulation_time) for i in range(stimulus_start, stimulus_end + 1)}\n",
    "\n",
    "# Generate Poisson spike trains for network neurons\n",
    "def generate_poisson_spike_train(rate, duration):\n",
    "    return np.sort(np.random.uniform(0, duration, np.random.poisson(rate * (duration / 1000))))\n",
    "\n",
    "network_spikes = {j: generate_poisson_spike_train(15, simulation_time) for j in range(num_network_neurons)}\n",
    "\n",
    "# Initialize Feedforward Synaptic Strengths\n",
    "feedforward_strengths = np.zeros((num_input_neurons, num_network_neurons))\n",
    "feedforward_strengths[stimulus_start:stimulus_end+1, :] = np.random.uniform(0, g_max, (stimulus_end - stimulus_start + 1, num_network_neurons))\n",
    "feedforward_strengths[:, 100:200] = 0  \n",
    "\n",
    "# LIF Neuron Membrane Potentials\n",
    "V = np.full(num_network_neurons, V_rest, dtype=np.float64)  \n",
    "g_ex = np.zeros(num_network_neurons)  # Excitatory conductance\n",
    "\n",
    "# STDP window function\n",
    "def stdp_window(delta_t):\n",
    "    return np.where(delta_t < 0, \n",
    "                    A_plus * np.exp(delta_t / tau_plus),  \n",
    "                    -A_minus * np.exp(-delta_t / tau_minus))  \n",
    "\n",
    "# Track spikes using a sparse dictionary\n",
    "spike_times = {j: [] for j in range(num_network_neurons)}\n",
    "\n",
    "# Precompute synaptic decay factor (avoids repeated `exp()` calls)\n",
    "synaptic_decay = np.exp(-dt / tau_ex)\n",
    "\n",
    "# Simulate LIF Neurons Over Time\n",
    "for t in range(simulation_time):\n",
    "    # Decay excitatory conductance\n",
    "    g_ex *= synaptic_decay  \n",
    "\n",
    "    # Compute feedforward inputs only for active neurons\n",
    "    pre_spiking = np.zeros(num_input_neurons)\n",
    "    for i in range(stimulus_start, stimulus_end + 1):  \n",
    "        if t in input_spikes[i]:  \n",
    "            pre_spiking[i] = 1  \n",
    "\n",
    "    g_ex += feedforward_strengths.T @ pre_spiking  \n",
    "\n",
    "    # Update membrane potential using LIF equation\n",
    "    V += (V_rest - V + g_ex * (E_ex - V)) / C_m * dt  \n",
    "\n",
    "    # Detect spiking neurons\n",
    "    spiking_neurons = np.where(V >= V_thresh)[0]  \n",
    "    for neuron in spiking_neurons:\n",
    "        spike_times[neuron].append(t)  # Store only spike times (sparse)\n",
    "\n",
    "    V[spiking_neurons] = V_reset  \n",
    "\n",
    "    # Apply STDP only for neurons that spiked\n",
    "    for j in spiking_neurons:\n",
    "        for i in range(stimulus_start, stimulus_end + 1):  \n",
    "            if t in input_spikes[i]:  \n",
    "                delta_t = t - np.array(spike_times[j])  # Faster computation\n",
    "                valid_times = delta_t[np.abs(delta_t) < time_window]  # Only valid pairs\n",
    "                \n",
    "                if valid_times.size > 0:  \n",
    "                    weight_change = np.sum(stdp_window(valid_times))  \n",
    "                    feedforward_strengths[i, j] += weight_change\n",
    "                    feedforward_strengths[i, j] = np.clip(feedforward_strengths[i, j], 0, g_max)\n",
    "\n",
    "# Normalize for Visualization\n",
    "feedforward_strengths_normalized = feedforward_strengths / g_max\n",
    "\n",
    "# PLOT \n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Get the coordinates and strengths of non-zero synapses\n",
    "y, x = np.where(feedforward_strengths_normalized > 0)  # y: input neurons, x: network neurons\n",
    "weights = feedforward_strengths_normalized[y, x]  # Synaptic strengths\n",
    "print(weights)\n",
    "\n",
    "# Plot the scatter points\n",
    "plt.scatter(x, y, c=weights, cmap='gray_r', s=10, marker='.', vmin=0, vmax=1)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Network Neuron')\n",
    "plt.ylabel('Input Neuron')\n",
    "plt.title('Feedforward Synaptic Strengths After STDP')\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(label='g/g_max')\n",
    "cbar.set_ticks([0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "\n",
    "# Set axis limits\n",
    "plt.xlim(0, 250)  # Network neurons\n",
    "plt.ylim(0, 1000)  # Input neurons\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load or Extract Feedforward Strengths from Figure 5A\n",
    "feedforward_strengths_normalized = np.random.uniform(0, 1, (num_input_neurons, num_network_neurons))  # Placeholder\n",
    "\n",
    "#Transfer Final Network Neuron Activity from 5A to 5B\n",
    "recurrent_strengths = feedforward_strengths_normalized.T @ feedforward_strengths_normalized  # Compute recurrent connections\n",
    "recurrent_strengths = recurrent_strengths / np.max(recurrent_strengths) * g_max  # Normalize\n",
    "\n",
    "\n",
    "# Apply STDP on Transferred Recurrent Connections\n",
    "for i in range(num_network_neurons):\n",
    "    for j in range(num_network_neurons):\n",
    "        if i != j and not (100 <= i < 200 and 100 <= j < 200):  # Prevent self-connections & enforce hole\n",
    "            pre_spikes = np.array(network_spikes[i])\n",
    "            post_spikes = np.array(network_spikes[j])\n",
    "\n",
    "            if pre_spikes.size > 0 and post_spikes.size > 0:\n",
    "                delta_t = pre_spikes[:, None] - post_spikes[None, :]\n",
    "                mask = np.abs(delta_t) < time_window\n",
    "                delta_t_windowed = delta_t[mask]\n",
    "\n",
    "                if delta_t_windowed.size > 0:\n",
    "                    updates = stdp_window(delta_t_windowed)\n",
    "                    weight_change = np.sum(updates)\n",
    "\n",
    "                    # **Ensure STDP does not modify w_ii**\n",
    "                    if i != j:\n",
    "                        recurrent_strengths[i, j] += weight_change\n",
    "                        recurrent_strengths[i, j] = np.clip(recurrent_strengths[i, j], 0, g_max)\n",
    "\n",
    "\n",
    "# **Enforce the hole again after STDP**\n",
    "recurrent_strengths[100:200, 100:200] = 0  \n",
    "# Same network neurons have no weight\n",
    "recurrent_strengths[0:250, 0:100] = 0  \n",
    "recurrent_strengths[0:250, 200:250] = 0  \n",
    "\n",
    "\n",
    "# Normalize \n",
    "recurrent_strengths_normalized = recurrent_strengths / g_max\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Get coordinates of non-zero recurrent synapses\n",
    "y, x = np.where(recurrent_strengths_normalized > 0)\n",
    "weights = recurrent_strengths_normalized[y, x]\n",
    "\n",
    "plt.scatter(x, y, c=weights, cmap='gray_r', s=10, marker='.', vmin=0, vmax=1)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Postsynaptic Network Neuron')\n",
    "plt.ylabel('Presynaptic Network Neuron')\n",
    "plt.title('Figure 5B')\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(label='g/g_max')\n",
    "cbar.set_ticks([0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "\n",
    "# Set axis limits\n",
    "plt.xlim(0, 250)\n",
    "plt.ylim(0, 250)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latest Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class STDP_Network:\n",
    "    def __init__(self, num_neurons=250, num_poisson=250, num_inputs=1000, dt=0.1,\n",
    "                 tau_pre=20.0, tau_post=20.0, tau_m=20.0, V_rest=-74.0, V_reset=-60.0,\n",
    "                 V_thresh=-54.0, C_m=0.9, g_leak=0.2, tau_s=5.0, \n",
    "                 A_plus_ff=0.005, A_minus_ff=0.005, A_plus_recur=0.001, A_minus_recur=0.001,\n",
    "                 B_ff=1.06, B_recur=1.04, g_max=1.0, poisson_rate=10, stimulus_width=100,\n",
    "                 mean_stim_time=100, R0=10, R1=80, sigma=100):\n",
    "        \"\"\"\n",
    "        Initialize the network with LIF neuron dynamics for the network neurons (not input neurons).\n",
    "        \"\"\"\n",
    "        self.num_neurons = num_neurons  # Postsynaptic (network) neurons\n",
    "        self.num_inputs = num_inputs    # Presynaptic (input) neurons\n",
    "        self.dt = dt\n",
    "        self.tau_pre = tau_pre\n",
    "        self.tau_post = tau_post\n",
    "        \n",
    "        # LIF neuron parameters\n",
    "        self.tau_m = tau_m  # Membrane time constant (ms)\n",
    "        self.V_rest = V_rest  # Resting potential (mV)\n",
    "        self.V_reset = V_reset  # Reset potential (mV)\n",
    "        self.V_thresh = V_thresh  # Threshold potential (mV)\n",
    "        self.C_m = C_m  # Membrane capacitance \n",
    "        self.g_leak = g_leak  # Leak conductance \n",
    "        self.E_ex = 0\n",
    "        self.tau_s = tau_s  # Synaptic time constant (ms)\n",
    "        self.mean_stim_time = mean_stim_time\n",
    "        self.R0 = R0\n",
    "        self.R1 = R1\n",
    "        self.sigma = sigma\n",
    "        # Feedforward learning rates and scaling factor\n",
    "        self.A_plus_ff = A_plus_ff\n",
    "        self.A_minus_ff = A_minus_ff\n",
    "        self.B_ff = B_ff\n",
    "        \n",
    "        # Recurrent learning rates and scaling factor\n",
    "        self.A_plus_recur = A_plus_recur\n",
    "        self.A_minus_recur = A_minus_recur\n",
    "        self.B_recur = B_recur\n",
    "        \n",
    "        self.g_max = g_max\n",
    "        \n",
    "        # Initialize feedforward weights: shape (num_inputs, num_neurons)\n",
    "        # self.ff_weights = np.random.rand(num_inputs, num_neurons) * g_max\n",
    "        #self.ff_weights = np.random.uniform(0.4, 1.0, (num_inputs, num_neurons))  # Stronger initial weights\n",
    "        # Initialize feedforward weights: shape (num_inputs, num_neurons)\n",
    "        # Define the center point (neuron 700)\n",
    "        center_neuron = 700\n",
    "\n",
    "        # Generate Gaussian distribution for weights with peak at neuron 700\n",
    "        weight_distribution = np.exp(-((np.arange(self.num_inputs) - center_neuron)**2) / (2 * stimulus_width**2))\n",
    "        weight_distribution = weight_distribution / np.max(weight_distribution)  # Normalize\n",
    "\n",
    "        # Initialize weights using this distribution, scaled by the desired maximum weight\n",
    "        self.ff_weights = np.random.uniform(0.6, 1.0, (self.num_inputs, self.num_neurons)) * weight_distribution[:, None]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Create and enforce sparsity mask (20% chance) on feedforward connections:\n",
    "        self.mask = (np.random.rand(num_inputs, num_neurons) < 0.2).astype(int)\n",
    "        self.ff_weights *= self.mask\n",
    "        \n",
    "        # Initialize recurrent weights (network-to-network); these start at 0.\n",
    "        self.recur_weights = np.zeros((num_neurons, num_neurons))\n",
    "\n",
    "        # Poisson neurons: here we use one per network neuron.\n",
    "        # Their spikes will directly update the excitatory conductance g_ex.\n",
    "        self.poisson_rate = poisson_rate\n",
    "        # g_ex: excitatory conductance for each network neuron.\n",
    "        self.g_ex = np.zeros(num_neurons)\n",
    "        \n",
    "        # Membrane potential for each network neuron\n",
    "        self.V_mem = np.full(num_neurons, V_rest)  # Initialize to resting potential\n",
    "        self.spike_train = np.zeros(num_neurons)  # Track spikes\n",
    "        \n",
    "        # Stimulus properties (for generating input spike patterns)\n",
    "        self.stimulus_width = stimulus_width\n",
    "        self.preferred_stimulus = np.linspace(0, 1000, num_inputs)\n",
    "        self.current_stimulus = None\n",
    "        \n",
    "        # Traces for feedforward updates:\n",
    "        self.pre_trace_feed = np.zeros(num_inputs)    # one per input neuron\n",
    "        self.post_trace_feed = np.zeros(num_neurons)    # one per network neuron\n",
    "        \n",
    "        # Traces for recurrent updates:\n",
    "        self.pre_trace_recur = np.zeros(num_neurons)      # for network neurons (presynaptic part of recurrent)\n",
    "        self.post_trace_recur = np.zeros(num_neurons)     # for network neurons (postsynaptic part of recurrent)\n",
    "    \n",
    "    def decay_traces(self):\n",
    "        \"\"\"Decay all traces over one time step.\"\"\"\n",
    "        self.pre_trace_feed *= np.exp(-self.dt / self.tau_pre)\n",
    "        self.post_trace_feed *= np.exp(-self.dt / self.tau_post)\n",
    "        self.pre_trace_recur *= np.exp(-self.dt / self.tau_pre)\n",
    "        self.post_trace_recur *= np.exp(-self.dt / self.tau_post)\n",
    "    \n",
    "    def update_poisson_input(self):\n",
    "        \"\"\"\n",
    "        Each network neuron receives input from one dedicated Poisson neuron.\n",
    "        For each network neuron, if its Poisson neuron fires (with probability poisson_rate*dt/1000),\n",
    "        g_ex is updated by a fixed factor (here, 0.096).\n",
    "        \"\"\"\n",
    "        poisson_spikes = np.random.rand(self.num_neurons) < (self.poisson_rate * self.dt / 1000.0)\n",
    "        self.g_ex = 0.5 * poisson_spikes.astype(float)  #0.096 in the paper but made it bigger \n",
    "        \n",
    "    def generate_stimulus(self, sim_time):\n",
    "        num_steps = int(sim_time / self.dt)\n",
    "        spike_array = np.zeros((num_steps, self.num_inputs), dtype=int)\n",
    "        current_step = 0\n",
    "        \n",
    "        while current_step < num_steps:\n",
    "            interval_duration = np.random.exponential(scale=self.mean_stim_time)\n",
    "            interval_steps = max(1, int(interval_duration / self.dt))\n",
    "            end_step = min(num_steps, current_step + interval_steps)\n",
    "            \n",
    "            s = np.random.uniform(1, 1000)\n",
    "            a_vals = np.arange(self.num_inputs)\n",
    "            rates = self.R0 + self.R1 * (\n",
    "                np.exp(-((s - a_vals) ** 2) / (2 * self.sigma ** 2)) +\n",
    "                np.exp(-((s + 1000 - a_vals) ** 2) / (2 * self.sigma ** 2)) +\n",
    "                np.exp(-((s - 1000 - a_vals) ** 2) / (2 * self.sigma ** 2))\n",
    "            )\n",
    "            \n",
    "            p = rates * self.dt / 1000\n",
    "            \n",
    "            for t in range(current_step, end_step):\n",
    "                spike_array[t, :] = (np.random.rand(self.num_inputs) < p).astype(int)\n",
    "            \n",
    "            current_step = end_step\n",
    "        \n",
    "        return spike_array\n",
    "\n",
    "\n",
    "    def update_membrane_potential(self, pre_spikes_feed, pre_spikes_recur):\n",
    "        \"\"\"\n",
    "        Update the membrane potential of each network neuron (LIF dynamics).\n",
    "        Includes:\n",
    "        - Exponential decay of excitatory conductance g_ex\n",
    "        - Updates from presynaptic spikes (both feedforward and recurrent)\n",
    "        - LIF membrane potential update\n",
    "        \"\"\"\n",
    "        # Decay excitatory conductance\n",
    "        self.g_ex *= np.exp(-self.dt / self.tau_s)\n",
    "        print(f\"g_ex before input spikes (first 10 neurons): {self.g_ex[:10]}\")\n",
    "\n",
    "        # Increase g_ex due to feedforward spikes\n",
    "        self.g_ex += np.dot(pre_spikes_feed, self.ff_weights)*3  # Input spikes to Network neurons\n",
    "\n",
    "        # Increase g_ex due to recurrent spikes\n",
    "        self.g_ex += np.dot(pre_spikes_recur, self.recur_weights)*3  # Recurrent spikes to Network neurons\n",
    "        print(f\"g_ex after input spikes (first 10 neurons): {self.g_ex[:10]}\")\n",
    "\n",
    "        # Compute synaptic current\n",
    "        synaptic_current = self.g_ex - self.g_leak * (self.V_mem - self.V_rest)\n",
    "        # Compute synaptic and leak currents\n",
    "        synaptic_current = self.g_ex * (self.E_ex - self.V_mem)  # Excitatory synaptic current\n",
    "        leak_current = self.g_leak * (self.V_rest - self.V_mem)  # Leak current\n",
    "        total_current = synaptic_current + leak_current\n",
    "\n",
    "        # Update membrane potential using LIF dynamics\n",
    "        dV = (synaptic_current / self.C_m) * (self.dt / self.tau_m)\n",
    "        self.V_mem += dV\n",
    "\n",
    "        # Check for spikes\n",
    "        spike = self.V_mem >= self.V_thresh\n",
    "\n",
    "        # Reset spiking neurons\n",
    "        self.V_mem[spike] = self.V_reset\n",
    "        self.spike_train[spike] = 1  # Mark spike occurrence\n",
    "\n",
    "    \n",
    "    def update_pre_feedforward(self, pre_spikes):\n",
    "        print(\"Updating pre feedforward weights...\")\n",
    "        print(\"Pre-spikes count:\", np.sum(pre_spikes))\n",
    "        self.pre_trace_feed[pre_spikes] += 1\n",
    "        print(\"Pre-trace (sample):\", self.pre_trace_feed[:10])  # Print first 10 values\n",
    "        \n",
    "        for i in np.where(pre_spikes)[0]:\n",
    "            self.ff_weights[i, :] -= self.B_ff * self.A_minus_ff * self.post_trace_feed * (self.mask[i, :] == 1)\n",
    "        self.ff_weights *= self.mask  # Enforce sparsity\n",
    "        self.ff_weights = np.clip(self.ff_weights, 0, self.g_max)\n",
    "\n",
    "    def update_post_feedforward(self, post_spikes):\n",
    "        print(\"Updating post feedforward weights...\")\n",
    "        print(\"Post-spikes count:\", np.sum(post_spikes))\n",
    "        self.post_trace_feed[post_spikes] += 1\n",
    "        print(\"Post-trace (sample):\", self.post_trace_feed[:10])  # Print first 10 values\n",
    "        \n",
    "        for j in np.where(post_spikes)[0]:\n",
    "            self.ff_weights[:, j] += self.B_ff * self.A_plus_ff * self.pre_trace_feed * (self.mask[:, j] == 1)\n",
    "        self.ff_weights *= self.mask\n",
    "        self.ff_weights = np.clip(self.ff_weights, 0, self.g_max)\n",
    "    \n",
    "    def update_recurrent_weights(self, pre_spikes, post_spikes):\n",
    "        \"\"\"\n",
    "        Update recurrent weights based on pre and post spikes.\n",
    "        \"\"\"\n",
    "        # Update pre-synaptic trace for neurons that spiked\n",
    "        self.pre_trace_recur[pre_spikes] += 1\n",
    "        \n",
    "        # Apply STDP depression (A_minus) for presynaptic spikes\n",
    "        for i in np.where(pre_spikes)[0]:\n",
    "            self.recur_weights[i, :] -= self.B_recur * self.A_minus_recur * self.post_trace_recur\n",
    "\n",
    "        # Update post-synaptic trace for neurons that spiked\n",
    "        self.post_trace_recur[post_spikes] += 1\n",
    "\n",
    "        # Apply STDP potentiation (A_plus) for postsynaptic spikes\n",
    "        for j in np.where(post_spikes)[0]:\n",
    "            self.recur_weights[:, j] += self.B_recur * self.A_plus_recur * self.pre_trace_recur\n",
    "        print(\"Before Clipping: Min weight =\", np.min(self.recur_weights))\n",
    "        self.recur_weights = np.clip(self.recur_weights, 0, self.g_max)\n",
    "        print(\"After Clipping: Min weight =\", np.min(self.recur_weights))\n",
    "\n",
    "        # Clip weights between 0 and g_max\n",
    "        #self.recur_weights = np.clip(self.recur_weights, 0, self.g_max)\n",
    "    \n",
    "    def simulate(self, T=500, feed_pre_times=[10, 50, 120], feed_post_times=[15, 55, 130],\n",
    "                recur_pre_times=[60, 140], recur_post_times=[65, 150]):\n",
    "        \"\"\"\n",
    "        Run the simulation for T time steps.\n",
    "        \"\"\"\n",
    "        weights_history_ff = []  # Track feedforward weights over time\n",
    "        weights_history_recur = []  # Track recurrent weights over time\n",
    "        \n",
    "        for t in range(T):\n",
    "            self.decay_traces()\n",
    "            self.update_poisson_input()\n",
    "            input_spikes = self.generate_stimulus(T)  # Generate input spikes\n",
    "\n",
    "            self.spike_train.fill(0)  # Reset spike train before each time step\n",
    "            print(f\"Input spikes (t={t}): {input_spikes}\")\n",
    "            print(f\"Time {t}: Input spikes count = {np.sum(input_spikes)}\")\n",
    "\n",
    "            # Extract pre-synaptic spike activity\n",
    "            #pre_spikes_feed = input_spikes.astype(bool)  # Input neurons\n",
    "            pre_spikes_feed = input_spikes[t]\n",
    "            pre_spikes_recur = self.spike_train.astype(bool)  # Network neurons (previous step)\n",
    "\n",
    "            # Update membrane potentials with proper conductance dynamics\n",
    "            self.update_membrane_potential(pre_spikes_feed, pre_spikes_recur)\n",
    "            # **Detect postsynaptic spikes in the current time step**\n",
    "            post_spikes = self.spike_train.astype(bool)\n",
    "            print(f\"Time {t}: {np.sum(post_spikes)} neurons spiked\")\n",
    "            print(f\"Time {t}: Membrane potential sample: {self.V_mem[:250]}\")\n",
    "            print(f\"Max V_mem at t={t}: {np.max(self.V_mem)}\")\n",
    "            print(f\"Time {t}: g_ex (first 10 neurons) = {self.g_ex[:10]}\")\n",
    "            # Update feedforward weights\n",
    "            if t in feed_pre_times:\n",
    "                self.update_pre_feedforward(pre_spikes_feed)\n",
    "                print(f\"Time {t}: Updating pre feedforward weights...\")\n",
    "            if t in feed_post_times:\n",
    "                self.update_post_feedforward(post_spikes)\n",
    "                print(f\"Time {t}: Updating post feedforward weights...\")\n",
    "\n",
    "            # Update recurrent weights\n",
    "            self.update_recurrent_weights(pre_spikes_recur, post_spikes)\n",
    "\n",
    "            # Store weight history\n",
    "            weights_history_ff.append(self.ff_weights.copy())\n",
    "            weights_history_recur.append(self.recur_weights.copy())\n",
    "\n",
    "        return np.array(weights_history_ff), np.array(weights_history_recur)\n",
    "\n",
    "\n",
    "# Run the simulation\n",
    "network = STDP_Network()\n",
    "ff_history, recur_history = network.simulate()\n",
    "\n",
    "# Plotting final feedforward and recurrent weight distributions\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(ff_history[-1], aspect='auto', cmap='gray_r',\n",
    "           extent=[0, network.num_neurons, 0, network.num_inputs])\n",
    "plt.colorbar(label=\"Feedforward Weight Strength\")\n",
    "plt.xlabel(\"Network Neuron\")\n",
    "plt.ylabel(\"Input Neuron\")\n",
    "plt.title(\"Final Feedforward Synaptic Weight Distribution\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(recur_history[-1], aspect='auto', cmap='gray',\n",
    "           extent=[0, network.num_neurons, 0, network.num_neurons])\n",
    "plt.colorbar(label=\"Recurrent Weight Strength\")\n",
    "plt.xlabel(\"Network Neuron (Postsynaptic)\")\n",
    "plt.ylabel(\"Network Neuron (Presynaptic)\")\n",
    "plt.title(\"Final Recurrent Synaptic Weight Distribution\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
