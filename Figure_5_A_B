import numpy as np
import matplotlib.pyplot as plt


class STDP_Network:
    def __init__(self, num_neurons=250, num_poisson=250, num_inputs=1000, dt=0.1,
                 tau_pre=20.0, tau_post=20.0, tau_m=20.0, V_rest=-74.0, V_reset=-60.0,
                 V_thresh=-54.0, C_m=0.9, g_leak=0.2, tau_s=5.0,
                 A_plus_ff=0.005, A_minus_ff=0.005, A_plus_recur=0.001, A_minus_recur=0.001,
                 B_ff=1.06, B_recur=1.04, g_max=1.0, poisson_rate=500, stimulus_width=100,
                 mean_stim_time=100, R0=10, R1=80, sigma=100, sim_time=100):
        """
        Initialize the network with LIF neuron dynamics for the network neurons (not input neurons).
        """
        self.num_neurons = num_neurons  # Postsynaptic (network) neurons
        self.num_inputs = num_inputs  # Presynaptic (input) neurons
        self.dt = dt
        self.tau_pre = tau_pre
        self.tau_post = tau_post

        # LIF neuron parameters
        self.tau_m = tau_m  # Membrane time constant (ms)
        self.V_rest = V_rest  # Resting potential (mV)
        self.V_reset = V_reset  # Reset potential (mV)
        self.V_thresh = V_thresh  # Threshold potential (mV)
        self.C_m = C_m  # Membrane capacitance
        self.g_leak = g_leak  # Leak conductance
        self.E_ex = 0
        self.tau_s = tau_s  # Synaptic time constant (ms)
        self.mean_stim_time = mean_stim_time
        self.R0 = R0
        self.R1 = R1
        self.sigma = sigma
        # Feedforward learning rates and scaling factor
        self.A_plus_ff = A_plus_ff
        self.A_minus_ff = A_minus_ff
        self.B_ff = B_ff
        self.sim_time = sim_time

        # Recurrent learning rates and scaling factor
        self.A_plus_recur = A_plus_recur
        self.A_minus_recur = A_minus_recur
        self.B_recur = B_recur

        self.g_max = g_max

        # Initialize feedforward weights: shape (num_inputs, num_neurons)
        # self.ff_weights = np.random.rand(num_inputs, num_neurons) * g_max
        # self.ff_weights = np.random.uniform(0.4, 1.0, (num_inputs, num_neurons))  # Stronger initial weights
        # Initialize feedforward weights: shape (num_inputs, num_neurons)
        # Define the center point (neuron 700)
        center_neuron = 700

        # Generate Gaussian distribution for weights with peak at neuron 700
        weight_distribution = np.exp(-((np.arange(self.num_inputs) - center_neuron) ** 2) / (2 * stimulus_width ** 2))
        weight_distribution = weight_distribution / np.max(weight_distribution)  # Normalize

        # Initialize weights using this distribution, scaled by the desired maximum weight
        self.ff_weights = np.random.uniform(0.6, 1.0, (self.num_inputs, self.num_neurons)) * weight_distribution[:, None]
        self.ff_weights[:, 100:201] = 0

        # Create and enforce sparsity mask (20% chance) on feedforward connections:
        self.mask = (np.random.rand(num_inputs, num_neurons) < 0.2).astype(int)
        self.mask[:, 100:201] = 0
        self.ff_weights *= self.mask

        # Initialize recurrent weights (network-to-network); these start at 0.
        self.recur_weights = np.zeros((num_neurons, num_neurons))

        # Poisson neurons: here we use one per network neuron.
        # Their spikes will directly update the excitatory conductance g_ex.
        self.poisson_rate = poisson_rate
        # g_ex: excitatory conductance for each network neuron.
        self.g_ex = np.zeros(num_neurons)

        # Membrane potential for each network neuron
        self.V_mem = np.full(num_neurons, V_rest)  # Initialize to resting potential
        self.spike_train = np.zeros(num_neurons)  # Track spikes

        # Stimulus properties (for generating input spike patterns)
        self.stimulus_width = stimulus_width
        self.preferred_stimulus = np.linspace(0, 1000, num_inputs)
        self.current_stimulus = None

        # Traces for feedforward updates per connection:
        self.pre_trace_feed = np.zeros((self.num_inputs, self.num_neurons))
        self.post_trace_feed = np.zeros((self.num_inputs, self.num_neurons))

        # Traces for recurrent updates per connection:
        self.pre_trace_recur = np.zeros((self.num_neurons, self.num_neurons))
        self.post_trace_recur = np.zeros((self.num_neurons, self.num_neurons))

    def decay_traces(self):
        """Decay all traces over one time step."""
        self.pre_trace_feed *= np.exp(-self.dt / self.tau_pre)
        self.post_trace_feed *= np.exp(-self.dt / self.tau_post)
        self.pre_trace_recur *= np.exp(-self.dt / self.tau_pre)
        self.post_trace_recur *= np.exp(-self.dt / self.tau_post)

    def update_poisson_input(self):
        """
        Each network neuron receives input from one dedicated Poisson neuron.
        For each network neuron, if its Poisson neuron fires (with probability poisson_rate*dt/1000),
        g_ex is updated by a fixed factor (here, 0.096).
        """
        poisson_spikes = np.random.rand(self.num_neurons) < (self.poisson_rate * (self.dt / 1000.0))
        self.g_ex += 0.096 * poisson_spikes.astype(int)

    def generate_stimulus(self, sim_time):
        num_steps = int(sim_time / self.dt)
        spike_array = np.zeros((num_steps, self.num_inputs), dtype=int)
        current_step = 0

        while current_step < num_steps:
            interval_duration = np.random.exponential(scale=self.mean_stim_time)
            interval_steps = max(1, int(interval_duration / self.dt))
            end_step = min(num_steps, current_step + interval_steps)

            s = np.random.uniform(1, 1000)
            a_vals = np.arange(self.num_inputs)
            rates = self.R0 + self.R1 * (
                    np.exp(-((s - a_vals) ** 2) / (2 * self.sigma ** 2)) +
                    np.exp(-((s + 1000 - a_vals) ** 2) / (2 * self.sigma ** 2)) +
                    np.exp(-((s - 1000 - a_vals) ** 2) / (2 * self.sigma ** 2))
            )

            p = rates * self.dt / 1000.0

            for t in range(current_step, end_step):
                spike_array[t, :] = (np.random.rand(self.num_inputs) < p).astype(int)

            current_step = end_step

        return spike_array

    def update_membrane_potential(self, pre_spikes_feed, pre_spikes_recur):
        """
        Update the membrane potential of each network neuron (LIF dynamics).
        Includes:
        - Exponential decay of excitatory conductance g_ex
        - Updates from presynaptic spikes (both feedforward and recurrent)
        - LIF membrane potential update
        """
        # Decay excitatory conductance
        self.g_ex *= np.exp(-self.dt / self.tau_s)
        print(f"g_ex before input spikes (first 10 neurons): {self.g_ex[:10]}")

        # Increase g_ex due to feedforward spikes
        self.g_ex += np.dot(pre_spikes_feed, self.ff_weights) # Input spikes to Network neurons

        # Increase g_ex due to recurrent spikes
        self.g_ex += np.dot(pre_spikes_recur, self.recur_weights)  # Recurrent spikes to Network neurons
        print(f"g_ex after input spikes (first 10 neurons): {self.g_ex[:10]}")

        # Update membrane potential using LIF dynamics
        dV = (1 / self.tau_m) * (self.V_rest - self.V_mem * (1 + self.g_ex))
        self.V_mem += dV

        # Check for spikes
        spike = self.V_mem >= self.V_thresh

        # Reset spiking neurons
        self.V_mem[spike] = self.V_reset
        self.spike_train[spike] = 1  # Mark spike occurrence

    def update_pre_feedforward(self, pre_spikes):
        print("Updating pre feedforward weights...")
        print("Pre-spikes count:", np.sum(pre_spikes))
        self.pre_trace_feed[pre_spikes] += 1
        print("Pre-trace (sample):", self.pre_trace_feed[:10])  # Print first 10 values

        for i in np.where(pre_spikes)[0]:  # Loop through presynaptic neurons that spiked
            self.ff_weights[i, :] -= self.B_ff * self.A_minus_ff * self.post_trace_feed[i, :] * (self.mask[i, :] == 1)
        self.ff_weights *= self.mask  # Enforce sparsity
        self.ff_weights = np.clip(self.ff_weights, 0, self.g_max)


    def update_post_feedforward(self, post_spikes):
        print("Updating post feedforward weights...")
        print("Post-spikes count:", np.sum(post_spikes))
        for j in np.where(post_spikes)[0]:  # Iterate over neurons that spiked
            self.post_trace_feed[:, j] += 1  # Update the post-synaptic trace for all input neurons
        print("Post-trace (sample):", self.post_trace_feed[:10])  # Print first 10 values

        for j in np.where(post_spikes)[0]:  # Iterate over neurons that spiked
            self.ff_weights[:, j] += self.B_ff * self.A_plus_ff * self.pre_trace_feed[:, j] * (self.mask[:, j] == 1)

        self.ff_weights *= self.mask
        self.ff_weights = np.clip(self.ff_weights, 0, self.g_max)

    def update_recurrent_weights(self, pre_spikes, post_spikes):
        """
        Update recurrent weights based on pre and post spikes.
        Returns:
        pre_depression: the weight change due to pre-synaptic (depression) updates.
        post_potentiation: the weight change due to post-synaptic (potentiation) updates.
        """
        # Initialize matrices to record changes
        pre_depression = np.zeros_like(self.recur_weights)
        post_potentiation = np.zeros_like(self.recur_weights)
        
        # Update pre-synaptic trace for neurons that spiked
        self.pre_trace_recur[pre_spikes] += 1
        
        # STDP Depression: For each presynaptic spike, subtract based on the corresponding post-trace.
        for i in np.where(pre_spikes)[0]:
            # Use only the i-th row of post_trace_recur
            delta = self.B_recur * self.A_minus_recur * self.post_trace_recur[i, :] 
            pre_depression[i, :] = delta
            self.recur_weights[i, :] -= delta

        # Update post-synaptic trace for neurons that spiked
        self.post_trace_recur[post_spikes] += 1
        
        # STDP Potentiation: For each postsynaptic spike, add based on the corresponding pre-trace.
        for j in np.where(post_spikes)[0]:
            # Use only the j-th column of pre_trace_recur
            delta = self.B_recur * self.A_plus_recur * self.pre_trace_recur[:, j]
            post_potentiation[:, j] = delta
            self.recur_weights[:, j] += delta

        # Clip the recurrent weights to the allowed range.
        self.recur_weights = np.clip(self.recur_weights, 0, self.g_max)
        
        return pre_depression, post_potentiation


    def simulate(self, T=100, feed_pre_times=[1, 3, 7], feed_post_times=[2, 5, 10],
                 recur_pre_times=[60, 140], recur_post_times=[65, 150]):
        """
        Run the simulation for T time steps.
        """
        weights_history_ff = []  # Track feedforward weights over time
        weights_history_recur = []  # Track recurrent weights over time
        network_spike_history = []  # Record network spikes over time
        recur_potentiation_history = []   # Post-synaptic (potentiation) contributions history
        recur_depression_history = []     # Pre-synaptic (depression) contributions history

        for t in range(T):
            self.decay_traces()
            self.update_poisson_input()
            input_spikes = self.generate_stimulus(T)  # Generate input spikes for the current time step

            self.spike_train.fill(0)  # Reset spike train before each time step
            print(f"Input spikes (t={t}): {input_spikes}")
            print(f"Time {t}: Input spikes count = {np.sum(input_spikes)}")

            # Extract pre-synaptic spike activity
            # pre_spikes_feed = input_spikes.astype(bool)  # Input neurons
            pre_spikes_feed = input_spikes[t]
            pre_spikes_recur = self.spike_train.astype(bool)  # Network neurons (previous step)

            #Network spike
            #n_spikes = self.generate_network_spikes(pre_spikes_feed, pre_spikes_recur)
            # Record the network spike train at this time step

            # Update membrane potentials with proper conductance dynamics
            self.update_membrane_potential(pre_spikes_feed, pre_spikes_recur)
            # **Detect postsynaptic spikes in the current time step**
            post_spikes = self.spike_train.astype(bool)
            print(f"Time {t}: {np.sum(post_spikes)} neurons spiked")
            print(f"Time {t}: Membrane potential sample: {self.V_mem[:250]}")
            print(f"Max V_mem at t={t}: {np.max(self.V_mem)}")
            print(f"Time {t}: g_ex (first 10 neurons) = {self.g_ex[:10]}")

            post_spikes_recurrent = self.spike_train.astype(bool)
            # Update feedforward weights
            if t in feed_pre_times:
                self.update_pre_feedforward(pre_spikes_feed)
                print(f"Time {t}: Updating pre feedforward weights...")
            if t in feed_post_times:
                self.update_post_feedforward(post_spikes)
                print(f"Time {t}: Updating post feedforward weights...")

            # Update recurrent weights and capture the contributions
            pre_dep, post_pot = self.update_recurrent_weights(post_spikes, post_spikes_recurrent)
            recur_depression_history.append(pre_dep.copy())
            recur_potentiation_history.append(post_pot.copy())
            network_spike_history.append(self.spike_train.copy())

            #network_spike_history.append(n_spikes)

            # Store weight history
            weights_history_ff.append(self.ff_weights.copy())
            weights_history_recur.append(self.recur_weights.copy())

        return (np.array(weights_history_ff),
                np.array(weights_history_recur),
                np.array(network_spike_history),
                np.array(recur_depression_history),
                np.array(recur_potentiation_history))

# Run the simulation
network = STDP_Network()
ff_history, recur_history, net_spike_history,recur_depression_history, potentiation_history= network.simulate()

spike_history = network.generate_stimulus(network.sim_time)
spike_times_list = [np.where(spike_history[:, neuron] == 1) [0]
                    for neuron in range(spike_history.shape[1])]

plt.figure(figsize=(10, 6))
plt.eventplot(spike_times_list, colors="black")
plt.xlabel("Time (ms)")
plt.ylabel("Input Neuron")
plt.title("Spike Train Event Plot of Input neurons")
plt.show()

network_spike_times_list = [
    np.where(net_spike_history[:, neuron] == 1)[0]
    for neuron in range(net_spike_history.shape[1])
]

plt.figure(figsize=(10, 6))
plt.eventplot(network_spike_times_list,colors="black")
plt.xlabel("Time (ms)")
plt.ylabel("Network Neuron")
plt.title("Spike Train Event Plot (Network Neurons (post feedforward connection))")
plt.show()

# Plotting final feedforward and recurrent weight distributions
plt.figure(figsize=(8, 6))
plt.imshow(ff_history[-1], aspect='auto', cmap='gray_r',
           extent=[0, network.num_neurons, 0, network.num_inputs])
plt.colorbar(label="Feedforward Weight Strength")
plt.xlabel("Network Neuron")
plt.ylabel("Input Neuron")
plt.title("Final Feedforward Synaptic Weight Distribution")
plt.show()

plt.figure(figsize=(8, 6))
plt.imshow(recur_history[-1], aspect='auto', cmap='gray_r',
           extent=[0, network.num_neurons, 0, network.num_neurons])
plt.colorbar(label="Recurrent Weight Strength")
plt.xlabel("Postsynaptic Neuron")
plt.ylabel("Presynaptic Neuron")
plt.title("Final Recurrent Synaptic Weight Distribution")
plt.show()

